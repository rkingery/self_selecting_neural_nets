{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from ss_perf_utils import *\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "global device,dtype\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy(X,y,layer_dims,num_iters,lr=0.01,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+np.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    W1 = np.random.randn(dh,din)\n",
    "    b1 = np.random.randn(dh,1)\n",
    "    W2 = np.random.randn(dout,dh)\n",
    "    b2 = np.random.randn(dout,1)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = np.dot(W1,X)+b1\n",
    "        A = Z1.clip(min=0) # relu\n",
    "        Z2 = np.dot(W2,A)+b2\n",
    "        yhat = sigmoid(Z2).clip(1e-6,1.-1e-6)\n",
    "    \n",
    "        loss = 1./m*(-np.dot(y,np.log(yhat).T)-np.dot(1-y,np.log(1-yhat).T))\n",
    "        loss = loss.squeeze().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(np.divide(y,yhat) - np.divide(1-y, 1-yhat))\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        dW2 = 1./m*np.dot(dZ2,A.T)\n",
    "        db2 = 1./m*np.sum(dZ2,1,keepdims=True)\n",
    "        dA = np.dot(W2.T,dZ2)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        dW1 = 1./m*np.dot(dZ1,X.T)\n",
    "        db1 = 1./m*np.sum(dZ1,1,keepdims=True)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_numpy(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_numpy(W1,b1,W2,b2,losses,epsilon,delta,\n",
    "                                              max_hidden_size,tau,prob)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(X,y,layer_dims,num_iters,lr=0.01,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+torch.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    W1 = torch.randn(dh, din, dtype=dtype, requires_grad=False, device=device)\n",
    "    b1 = torch.randn(dh, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    W2 = torch.randn(dout, dh, dtype=dtype, requires_grad=False, device=device)\n",
    "    b2 = torch.randn(dout, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = torch.mm(W1,X)+b1\n",
    "        A = Z1.clamp(min=0) # relu\n",
    "        Z2 = torch.mm(W2,A)+b2\n",
    "        yhat = sigmoid(Z2).clamp(1e-6,1.-1e-6)\n",
    "    \n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(yhat,y)\n",
    "        loss = loss.squeeze_().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(torch.div(y,yhat) - torch.div(1-y, 1-yhat))\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        dW2 = 1./m*torch.mm(dZ2,A.t())\n",
    "        db2 = 1./m*torch.sum(dZ2,1,keepdim=True)\n",
    "        dA = torch.mm(W2.t(),dZ2)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        dW1 = 1./m*torch.mm(dZ1,X.t())\n",
    "        db1 = 1./m*torch.sum(dZ1,1,keepdim=True)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_pytorch(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_pytorch(W1,b1,W2,b2,losses,epsilon,delta,max_hidden_size,tau,prob,device)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 10000\n",
    "num_samples = 1000\n",
    "num_features = 2\n",
    "num_hidden = 100\n",
    "num_classes = 1\n",
    "lr = 0.1\n",
    "layer_dims = [num_features,num_hidden,num_classes]\n",
    "\n",
    "X,y,x1,x2 = gen_data(samples=num_samples,var=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after iteration 0: 5.490647\n",
      "loss after iteration 500: 0.027052\n",
      "loss after iteration 1000: 0.013905\n",
      "loss after iteration 1500: 0.009344\n",
      "loss after iteration 2000: 0.007019\n",
      "loss after iteration 2500: 0.005496\n",
      "loss after iteration 3000: 0.004309\n",
      "loss after iteration 3500: 0.003697\n",
      "loss after iteration 4000: 0.003256\n",
      "loss after iteration 4500: 0.002912\n",
      "loss after iteration 5000: 0.002634\n",
      "loss after iteration 5500: 0.002404\n",
      "loss after iteration 6000: 0.002211\n",
      "loss after iteration 6500: 0.002047\n",
      "loss after iteration 7000: 0.001905\n",
      "loss after iteration 7500: 0.001781\n",
      "loss after iteration 8000: 0.001672\n",
      "loss after iteration 8500: 0.001576\n",
      "loss after iteration 9000: 0.001490\n",
      "loss after iteration 9500: 0.001413\n",
      "time for numpy = 128.919129\n"
     ]
    }
   ],
   "source": [
    "X_np = X.T\n",
    "y_np = y.reshape(1,-1)\n",
    "\n",
    "tin = time.clock()\n",
    "losses,num_neurons = train_numpy(X_np,y_np,layer_dims,num_iters,lr=lr,add_del=False)\n",
    "tout = time.clock()\n",
    "tdiff = tout-tin\n",
    "print('\\ntime for numpy = %f' % tdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after iteration 0: 3.980994\n",
      "loss after iteration 500: 0.026215\n",
      "loss after iteration 1000: 0.013299\n",
      "loss after iteration 1500: 0.008580\n",
      "loss after iteration 2000: 0.006365\n",
      "loss after iteration 2500: 0.005047\n",
      "loss after iteration 3000: 0.004166\n",
      "loss after iteration 3500: 0.003537\n",
      "loss after iteration 4000: 0.003071\n",
      "loss after iteration 4500: 0.002710\n",
      "loss after iteration 5000: 0.002425\n",
      "loss after iteration 5500: 0.002196\n",
      "loss after iteration 6000: 0.002006\n",
      "loss after iteration 6500: 0.001849\n",
      "loss after iteration 7000: 0.001715\n",
      "loss after iteration 7500: 0.001599\n",
      "loss after iteration 8000: 0.001498\n",
      "loss after iteration 8500: 0.001409\n",
      "loss after iteration 9000: 0.001330\n",
      "loss after iteration 9500: 0.001260\n",
      "time for pytorch = 9.194320\n"
     ]
    }
   ],
   "source": [
    "X_pt = torch.tensor(X,device=device,dtype=dtype).t()\n",
    "y_pt = torch.tensor(y,device=device,dtype=dtype).reshape(1,-1)\n",
    "\n",
    "tin = time.clock()\n",
    "losses,num_neurons = train_pytorch(X_pt,y_pt,layer_dims,num_iters,lr=lr,add_del=False)\n",
    "tout = time.clock()\n",
    "tdiff = tout-tin\n",
    "print('\\ntime for pytorch = %f' % tdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses = np.array(losses)\n",
    "#filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "#filt_neurons[filt_neurons<1] = num_hidden\n",
    "\n",
    "#plt.plot(losses,color='blue')\n",
    "#plt.title('Loss')\n",
    "#plt.show()\n",
    "\n",
    "#plt.plot(filt_neurons,color='green')\n",
    "#plt.title('# Neurons')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
